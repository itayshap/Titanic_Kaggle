{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itayshap/Titanic_Kaggle/blob/main/MLOpsFinalProj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLOPS FINAL PROJECT\n",
        "###Submmited by:\n",
        "Baoz \n",
        "Omri\n",
        "Adam\n",
        "Itay\n"
      ],
      "metadata": {
        "id": "p5x-JgoSqSJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "qzX04aAWqbho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Boston Dataset"
      ],
      "metadata": {
        "id": "AXL2AQ9H7fJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing boston dataset\n",
        "from sklearn.datasets import load_boston\n",
        "boston = load_boston()\n",
        "boston_df = pd.DataFrame(boston.data)\n",
        "boston_df.columns=boston.feature_names\n",
        "boston_df['PRICE'] = boston.target \n",
        "boston_baseline_hyperparams = {}"
      ],
      "metadata": {
        "id": "N6OtVePh7XVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading French Dataset"
      ],
      "metadata": {
        "id": "HnbHG1rtBdO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv(\"https://www.openml.org/data/get_csv/20649148/freMTPL2freq.arff\",\n",
        "                 quotechar=\"'\")\n",
        "# rename column names '\"name\"' => 'name' \n",
        "df2.rename(lambda x: x.replace('\"', ''), axis='columns', inplace=True)\n",
        "df2['IDpol'] = df2['IDpol'].astype(np.int64)\n",
        "french_hyperparams = {'eval_metric':'poisson-nloglik',\n",
        "                      'objective': 'count:poisson',\n",
        "                      'colsample_bytree':0.9 ,\n",
        "                      'learning_rate':0.1,\n",
        "                      'max_depth':3,\n",
        "                      'min_child_weight':1,\n",
        "                      'reg_alpha':5,\n",
        "                      'subsample':0.9 }"
      ],
      "metadata": {
        "id": "LWv-1Ige-hse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating Generic Pipeline"
      ],
      "metadata": {
        "id": "263ipXqBB3Er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class model_pipeline():\n",
        "  def __init__(self, df, target_column, split_ratio, params):\n",
        "    self.UNIQUE_VALS_THRESHOLD = 9\n",
        "    self.model = XGBRegressor() if params == {} else XGBRegressor(params)\n",
        "    self.df = df\n",
        "    self.target_column = target_column\n",
        "    self.split_ratio = split_ratio \n",
        "    self.categorical_features, self.numeric_features = self.find_num_cat_features(df)\n",
        "    self.X_train, self.X_test, self.y_train, self.y_test = self.split_data(df)\n",
        "\n",
        "\n",
        "  def find_num_cat_features(self,df):\n",
        "    df_nunique = df.select_dtypes(include=['object']).nunique()\n",
        "    categorical_features = list(df_nunique[df_nunique < self.UNIQUE_VALS_THRESHOLD].index)\n",
        "    numeric_features = df.columns.drop(categorical_features).to_list()\n",
        "    numeric_features.remove(self.target_column)\n",
        "    return categorical_features, numeric_features\n",
        "\n",
        "  def split_data(self, df):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df.drop([self.target_column], axis = 1), \\\n",
        "                                                        df[self.target_column], test_size = self.split_ratio, random_state = 10)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "  \n",
        "  def create_pipeline(self):  \n",
        "    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
        "    categorical_transformer = Pipeline(steps=[('encoder', OneHotEncoder())])\n",
        "\n",
        "    # RFESelector_transformer = Pipeline(steps=[('RFESelector', RFESelector(10, self.model))])\n",
        "    RFESelector_transformer = Pipeline(steps=[('RFESelector', RFE(10, self.model))])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "      transformers=[\n",
        "        ('numeric', numeric_transformer, self.numeric_features),\n",
        "        ('all', RFESelector_transformer, self.X_train.columns.to_list()),\n",
        "        ('categorical', categorical_transformer, self.categorical_features)\n",
        "    ]) \n",
        "\n",
        "    self.pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                ('regressor', self.model)])\n",
        "    \n",
        "  def print_metrics(self, target, y_pred):\n",
        "    print('R^2 =',metrics.r2_score(target, y_pred))\n",
        "    print('MAE =',metrics.mean_absolute_error(target, y_pred))\n",
        "    print('MSE =',metrics.mean_squared_error(target, y_pred))\n",
        "    print('RMSE =',np.sqrt(metrics.mean_squared_error(target, y_pred)))\n",
        "\n",
        "\n",
        "  def run_pipeline(self):\n",
        "    self.pipeline.fit(self.X_train, self.y_train)\n",
        "\n",
        "  def predict(self, mode):\n",
        "    if mode == \"train\":\n",
        "      y_pred = self.pipeline.predict(self.X_train)\n",
        "      self.print_metrics(self.y_train, y_pred)\n",
        "    else:\n",
        "      y_pred = self.pipeline.predict(self.X_test)\n",
        "      self.print_metrics(self.y_test, y_pred)\n",
        "      "
      ],
      "metadata": {
        "id": "sQ-1oCDTRDX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_pipeline(boston_df, \"PRICE\", 0.4, boston_baseline_hyperparams)\n",
        "model.create_pipeline()\n",
        "model.run_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "2SJ4t37LUnle",
        "outputId": "7cfef687-82c1-41a1-ff65-0de6deb93192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a36253d99cab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboston_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PRICE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboston_baseline_hyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-5e18d757ed06>\u001b[0m in \u001b[0;36mcreate_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# RFESelector_transformer = Pipeline(steps=[('RFESelector', RFESelector(10, self.model))])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mRFESelector_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RFESelector'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRFE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     preprocessor = ColumnTransformer(\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() takes 2 positional arguments but 3 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(boston_df.drop(\"PRICE\", axis = 1), boston_df[\"PRICE\"], test_size = 0.4, random_state = 10)"
      ],
      "metadata": {
        "id": "Sq3ITJsOGNHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RFESelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_features, model):\n",
        "      self.n_features = n_features\n",
        "      self.model = model\n",
        "    #  self.important_features = []\n",
        "        \n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "      selector = RFE(self.model, n_features_to_select=self.n_features)\n",
        "      selector.fit(X, y)\n",
        "      self.important_features = selector.support_\n",
        "\n",
        "\n",
        "    def transform(self, X):\n",
        "      return X.loc[:, self.important_features]"
      ],
      "metadata": {
        "id": "5fd5lHKrkVra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XGBSelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_features, model):\n",
        "      self.n_features = n_features\n",
        "      self.model = XGB \n",
        "    #  self.important_features = []\n",
        "        \n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "      \n",
        "      my_dict = w.get_booster().get_score()\n",
        "      sorted(my_dict, key=my_dict.get, reverse=True)[:10]\n",
        "\n",
        "      selector = RFE(self.model, n_features_to_select=self.n_features)\n",
        "      selector.fit(X, y)\n",
        "      self.important_features = selector.support_\n",
        "\n",
        "\n",
        "    def transform(self, X):\n",
        "      return X.loc[:, self.important_features]"
      ],
      "metadata": {
        "id": "I6mTnwgIG1Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ydata_synthetic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l9MpQfkJHQKw",
        "outputId": "043155db-a024-4221-e6c6-d45344d87dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ydata_synthetic\n",
            "  Downloading ydata_synthetic-0.8.0-py2.py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting pytest==6.2.*\n",
            "  Downloading pytest-6.2.5-py3-none-any.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 8.0 MB/s \n",
            "\u001b[?25hCollecting numpy==1.23.*\n",
            "  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.1 MB 68.0 MB/s \n",
            "\u001b[?25hCollecting matplotlib==3.5.*\n",
            "  Downloading matplotlib-3.5.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 50.7 MB/s \n",
            "\u001b[?25hCollecting typeguard==2.13.*\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Collecting scikit-learn==1.1.*\n",
            "  Downloading scikit_learn-1.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting pandas==1.4.*\n",
            "  Downloading pandas-1.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.7 MB 70.8 MB/s \n",
            "\u001b[?25hCollecting pmlb==1.0.*\n",
            "  Downloading pmlb-1.0.1.post3-py3-none-any.whl (19 kB)\n",
            "Collecting tensorflow==2.9.0\n",
            "  Downloading tensorflow-2.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 167 bytes/s \n",
            "\u001b[?25hCollecting easydict==1.9\n",
            "  Downloading easydict-1.9.tar.gz (6.4 kB)\n",
            "Requirement already satisfied: tqdm<5.0 in /usr/local/lib/python3.8/dist-packages (from ydata_synthetic) (4.64.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.*->ydata_synthetic) (2.8.2)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 47.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.*->ydata_synthetic) (7.1.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.*->ydata_synthetic) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.*->ydata_synthetic) (21.3)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.*->ydata_synthetic) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.*->ydata_synthetic) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.4.*->ydata_synthetic) (2022.6)\n",
            "Collecting requests>=2.24.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from pmlb==1.0.*->ydata_synthetic) (6.0)\n",
            "Collecting iniconfig\n",
            "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from pytest==6.2.*->ydata_synthetic) (0.10.2)\n",
            "Collecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.8/dist-packages (from pytest==6.2.*->ydata_synthetic) (1.11.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from pytest==6.2.*->ydata_synthetic) (22.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.1.*->ydata_synthetic) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.1.*->ydata_synthetic) (1.7.3)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.1.*->ydata_synthetic) (1.2.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (2.9.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (1.51.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (2.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (2.9.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (4.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (2.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (57.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (3.19.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (1.12)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (14.0.6)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (0.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (1.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0->ydata_synthetic) (0.28.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.0->ydata_synthetic) (0.38.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.24.0->pmlb==1.0.*->ydata_synthetic) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.24.0->pmlb==1.0.*->ydata_synthetic) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.24.0->pmlb==1.0.*->ydata_synthetic) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.24.0->pmlb==1.0.*->ydata_synthetic) (2.10)\n",
            "Collecting scipy>=1.3.2\n",
            "  Downloading scipy-1.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 33.8 MB 319 kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0->ydata_synthetic) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0->ydata_synthetic) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0->ydata_synthetic) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0->ydata_synthetic) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0->ydata_synthetic) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0->ydata_synthetic) (2.15.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0->ydata_synthetic) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0->ydata_synthetic) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0->ydata_synthetic) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0->ydata_synthetic) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.0->ydata_synthetic) (5.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.0->ydata_synthetic) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0->ydata_synthetic) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0->ydata_synthetic) (3.2.2)\n",
            "Building wheels for collected packages: easydict\n",
            "  Building wheel for easydict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for easydict: filename=easydict-1.9-py3-none-any.whl size=6360 sha256=14e0a85e39ccf9d1b9460c6d4ccfca2d77d95ffeb02734cc54110f21c1ca923f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/e0/e9/305e348717e399665119bd012510d51ff4f22d709ff60c3096\n",
            "Successfully built easydict\n",
            "Installing collected packages: requests, numpy, scipy, pluggy, pandas, iniconfig, fonttools, typeguard, tensorflow, scikit-learn, pytest, pmlb, matplotlib, easydict, ydata-synthetic\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 2.7.1\n",
            "    Uninstalling typeguard-2.7.1:\n",
            "      Successfully uninstalled typeguard-2.7.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: easydict\n",
            "    Found existing installation: easydict 1.10\n",
            "    Uninstalling easydict-1.10:\n",
            "      Successfully uninstalled easydict-1.10\n",
            "Successfully installed easydict-1.9 fonttools-4.38.0 iniconfig-1.1.1 matplotlib-3.5.3 numpy-1.23.5 pandas-1.4.4 pluggy-1.0.0 pmlb-1.0.1.post3 pytest-6.2.5 requests-2.28.1 scikit-learn-1.1.3 scipy-1.9.3 tensorflow-2.9.0 typeguard-2.13.3 ydata-synthetic-0.8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pandas",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3qwXdn1LHYvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from ydata_synthetic.synthesizers.regular import RegularSynthesizer\n",
        "from ydata_synthetic.synthesizers import ModelParameters, TrainParameters\n",
        "\n",
        "#Load data and define the data processor parameters\n",
        "data = fetch_data('adult')\n",
        "num_cols = ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
        "cat_cols = ['workclass','education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
        "            'native-country', 'target']\n",
        "\n",
        "# DRAGAN training\n",
        "#Defining the training parameters of DRAGAN\n",
        "\n",
        "noise_dim = 128\n",
        "dim = 128\n",
        "batch_size = 500\n",
        "\n",
        "log_step = 100\n",
        "epochs = 500+1\n",
        "learning_rate = 1e-5\n",
        "beta_1 = 0.5\n",
        "beta_2 = 0.9\n",
        "models_dir = '../cache'\n",
        "\n",
        "gan_args = ModelParameters(batch_size=batch_size,\n",
        "                           lr=learning_rate,\n",
        "                           betas=(beta_1, beta_2),\n",
        "                           noise_dim=noise_dim,\n",
        "                           layers_dim=dim)\n",
        "\n",
        "train_args = TrainParameters(epochs=epochs,\n",
        "                             sample_interval=log_step)\n",
        "\n",
        "synth = RegularSynthesizer(modelname='dragan', model_parameters=gan_args, n_discriminator=3)\n",
        "synth.fit(data = data, train_arguments = train_args, num_cols = num_cols, cat_cols = cat_cols)\n",
        "\n",
        "synth.save('adult_synth.pkl')\n",
        "\n",
        "#########################################################\n",
        "#    Loading and sampling from a trained synthesizer    #\n",
        "#########################################################\n",
        "synthesizer = RegularSynthesizer.load('adult_synth.pkl')\n",
        "synthesizer.sample(1000)"
      ],
      "metadata": {
        "id": "muNKR3tDHKiH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "1158c1a4-2527-4aea-833d-86845fc2d91d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-d0104165f002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Load data and define the data processor parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'adult'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mnum_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fnlwgt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'capital-gain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'capital-loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hours-per-week'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m cat_cols = ['workclass','education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fetch_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive implementation of freeAI tool, later it will be added to pipeline"
      ],
      "metadata": {
        "id": "IgRZuiCJTrXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding to our data a binary column of correct/ incorrect classification\n",
        "def pre_process_FreeAI(model, X, y, threshold=0.1):\n",
        "  predictions = model.predict(X)\n",
        "  correct_classification = pd.Series(np.where(abs(predictions - y) <= threshold * y, True, False))\n",
        "  return pd.concat((X, correct_classification), axis=1)"
      ],
      "metadata": {
        "id": "MlVWgaN1TlX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing pre_proccess_freeAI\n",
        "import xgboost as XGBClassifier\n",
        "X = boston_df.drop(['PRICE'], axis=1)\n",
        "y = boston_df['PRICE']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.4, random_state = 10)\n",
        "xgbr = XGBClassifier.XGBRegressor(verbosity=0)\n",
        "xgbr.fit(X_train, y_train)\n",
        "new_X = pre_process_FreeAi(xgbr, X_train, y_train, threshold=0.1)\n",
        "new_X.rename(columns = {0:'accuracy'}, inplace = True)"
      ],
      "metadata": {
        "id": "5mDpU-HvTnvq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "e84f7564-3fec-4595-a7be-6c9dac988848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-20be7c2145e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mxgbr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mxgbr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnew_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process_FreeAi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgbr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mnew_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pre_process_FreeAi' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dtreeviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TO6ydnEEXUA8",
        "outputId": "898c6b74-aa24-4b54-dea7-d9fc0cac0a07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dtreeviz\n",
            "  Downloading dtreeviz-1.4.1-py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 799 kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from dtreeviz) (3.2.2)\n",
            "Requirement already satisfied: graphviz>=0.9 in /usr/local/lib/python3.8/dist-packages (from dtreeviz) (0.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from dtreeviz) (1.0.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.8/dist-packages (from dtreeviz) (3.6.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from dtreeviz) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from dtreeviz) (1.3.5)\n",
            "Collecting colour\n",
            "  Downloading colour-0.1.5-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->dtreeviz) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->dtreeviz) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->dtreeviz) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->dtreeviz) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->dtreeviz) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->dtreeviz) (2022.6)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.8/dist-packages (from pytest->dtreeviz) (1.4.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest->dtreeviz) (9.0.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from pytest->dtreeviz) (22.1.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from pytest->dtreeviz) (1.11.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.8/dist-packages (from pytest->dtreeviz) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from pytest->dtreeviz) (57.4.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->dtreeviz) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->dtreeviz) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->dtreeviz) (3.1.0)\n",
            "Installing collected packages: colour, dtreeviz\n",
            "Successfully installed colour-0.1.5 dtreeviz-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## needs refining for boston data\n",
        "\n",
        "import plotly.express as px\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# XGBoost\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# HDP\n",
        "import scipy.stats.kde as kde\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# DT\n",
        "import graphviz\n",
        "from dtreeviz.trees import dtreeviz\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz, _tree\n",
        "import itertools\n",
        "\n",
        "#################### HDP ########################\n",
        "def hpd_grid(sample, alpha=0.05, roundto=2, percent=0.5, show_plot=False):\n",
        "    \"\"\"Calculate highest posterior density (HPD) of array for given alpha.\n",
        "    The HPD is the minimum width Bayesian credible interval (BCI).\n",
        "    The function works for multimodal distributions, returning more than one mode\n",
        "    Parameters\n",
        "    ----------\n",
        "    sample : Numpy array or python list\n",
        "        An array containing MCMC samples\n",
        "    alpha : float\n",
        "        Desired probability of type I error (defaults to 0.05)\n",
        "    roundto: integer\n",
        "        Number of digits after the decimal point for the results\n",
        "    percent: float\n",
        "        Perecent of data in the highest density region\n",
        "    show_plots: bool\n",
        "        if true, will show intermediary plots\n",
        "    Returns\n",
        "    ----------\n",
        "    hpd: list with the highest density interval\n",
        "    x: array with grid points where the density was evaluated\n",
        "    y: array with the density values\n",
        "    modes: list listing the values of the modes\n",
        "    Src: https://github.com/aloctavodia/BAP/blob/master/first_edition/code/Chp1/hpd.py\n",
        "    Note this was modified to find low-accuracy areas\n",
        "    \"\"\"\n",
        "\n",
        "    # data points that create a density plot when histogramed\n",
        "    sample = np.asarray(sample)\n",
        "    sample = sample[~np.isnan(sample)]\n",
        "    if show_plot: px.histogram(sample, title='Histogram of Bi-Modal Data', template='simple_white', color_discrete_sequence=['#177BCD']).show()\n",
        "\n",
        "    # get upper and lower bounds on search space\n",
        "    l = np.min(sample)\n",
        "    u = np.max(sample)\n",
        "\n",
        "    # get x-axis values\n",
        "    x = np.linspace(l, u, 2000)\n",
        "\n",
        "    # get kernel density estimate\n",
        "    density = kde.gaussian_kde(sample)\n",
        "    y = density.evaluate(x)\n",
        "\n",
        "    if show_plot: px.scatter(x=x, y=y, title='Density Estimate of Bi-Modal Data', template='simple_white').update_traces(marker=dict(color='#177BCD')).show()\n",
        "\n",
        "    # sort by size of y (density estimate), descending \n",
        "    xy_zipped = zip(x, y/np.sum(y))\n",
        "    xy = sorted(xy_zipped, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # get all x's where y is in the top 1-alpha percent\n",
        "    # this is to bound the type 1 error\n",
        "    xy_cum_sum = 0\n",
        "    hdv = [] # x values\n",
        "    for val in xy:\n",
        "        xy_cum_sum += val[1]\n",
        "        hdv.append(val[0])\n",
        "        if xy_cum_sum >= (1-alpha):\n",
        "            break\n",
        "\n",
        "    # determine horizontal line corresponding to percent \n",
        "    yy_zipped = zip(y, y/np.sum(y))\n",
        "    yy = sorted(yy_zipped, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    y_cum_sum = 0\n",
        "    y_cutoff = 0\n",
        "    for val in yy:\n",
        "        y_cum_sum += val[1]\n",
        "        if y_cum_sum >= percent:\n",
        "            y_cutoff = val[0]\n",
        "            break\n",
        "\n",
        "    # get indices of sample in range \n",
        "    intersections = []\n",
        "    for i, curr in enumerate(y):\n",
        "        prior = y[i-1]\n",
        "        if (prior < y_cutoff and curr >= y_cutoff) or (prior >= y_cutoff and curr < y_cutoff):\n",
        "            intersections.append(x[i])\n",
        "\n",
        "    indices = []\n",
        "    for i in range(0, len(intersections), 2):\n",
        "        lower, upper = intersections[i], intersections[i+1]\n",
        "        indices.append([i for i,v in enumerate(sample) if v <= upper and v >= lower])\n",
        "\n",
        "    # setup for difference comparison\n",
        "    hdv.sort()\n",
        "    diff = (u-l)/20  # differences of 5%\n",
        "    hpd = []\n",
        "    hpd.append(round(min(hdv), roundto))\n",
        "\n",
        "    # if y_i - y_{i-1} > diff then save\n",
        "    for i in range(1, len(hdv)):\n",
        "        if hdv[i]-hdv[i-1] >= diff:\n",
        "            hpd.append(round(hdv[i-1], roundto))\n",
        "            hpd.append(round(hdv[i], roundto))\n",
        "    hpd.append(round(max(hdv), roundto))\n",
        "\n",
        "    # prepare to calcualte value with highest density\n",
        "    ite = iter(hpd)\n",
        "    hpd = list(zip(ite, ite)) # create sequential pairs\n",
        "    modes = []\n",
        "\n",
        "    # find x and y value whith highest density\n",
        "    for value in hpd:\n",
        "         x_hpd = x[(x > value[0]) & (x < value[1])]\n",
        "         y_hpd = y[(x > value[0]) & (x < value[1])]\n",
        "         modes.append(round(x_hpd[np.argmax(y_hpd)], roundto)) # store x-value where density is highest in range\n",
        "    return (hpd, x, y, modes, y_cutoff, np.array(indices).flatten())\n",
        "\n",
        "\n",
        "############ HDP example ##############\n",
        "\n",
        "def hpd_example(show_plot=False): \n",
        "    \"\"\" Plot an example of HDP method on bimodal data.\n",
        "    Src: https://stackoverflow.com/questions/53671925/highest-density-interval-hdi-for-posterior-distribution-pystan\n",
        "    \"\"\"\n",
        "\n",
        "    # include two modes\n",
        "    samples = np.random.normal(loc=[-4,4], size=(1000, 2)).flatten()\n",
        "\n",
        "    # compute high density regions\n",
        "    hpd_mu, x_mu, y_mu, modes_mu, y_cutoff, indices = hpd_grid(samples, show_plot=False)\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "\n",
        "    # raw data\n",
        "    plt.hist(samples, density=True, bins=29, alpha=0.5)\n",
        "\n",
        "    # estimated distribution\n",
        "    plt.plot(x_mu, y_mu)\n",
        "    plt.title('Highest Prior Density Region for Bi-Modal Data')\n",
        "\n",
        "    # high density intervals\n",
        "    for (x0, x1) in hpd_mu:\n",
        "        plt.hlines(y=0, xmin=x0, xmax=x1, linewidth=5)\n",
        "        plt.axvline(x=x0, color='grey', linestyle='--', linewidth=1)\n",
        "        plt.axvline(x=x1, color='grey', linestyle='--', linewidth=1)\n",
        "\n",
        "    # modes\n",
        "    for xm in modes_mu:\n",
        "        plt.axvline(x=xm, color='r')\n",
        "\n",
        "    # 95% of data\n",
        "    plt.axhline(y=y_cutoff, color='g')\n",
        "\n",
        "    if show_plot: plt.show()\n",
        "\n",
        "def hpd_iterative_search(col, accuracy, start_percent=0.5, end_percent=0.98, increment=0.05, acc_cutoff=-0.005):\n",
        "    \"\"\"\n",
        "    :param col: (pd.Series) univariate numeric col to search through\n",
        "    :param accuracy: (pd.Series) boolean accuracy column\n",
        "    :param start_percent: (flaot) percent to start with\n",
        "    :param end_percent: (flaot) percent to end with\n",
        "    :param increment: (float) value to increment by\n",
        "    :param acc_cutoff: (float) accuracy cutoff to return data\n",
        "    :return: (2d arry) of indices of problematic areas\n",
        "    \"\"\"\n",
        "\n",
        "    out = [] \n",
        "    \n",
        "    prior_indices = {} \n",
        "    prior_acc = None\n",
        "    prior_p = None \n",
        "    percents = np.arange(start_percent, end_percent, increment)[::-1] \n",
        "\n",
        "    # get smaller and smaller data slices\n",
        "    for p in percents:\n",
        "        # run HDP\n",
        "        *_, indices = hpd_grid(col, percent=p)\n",
        "\n",
        "        if indices.shape[0] != 0:\n",
        "\n",
        "            # get accuracy for HDP\n",
        "            indices = indices[0] if indices.shape[0] < 10 else indices\n",
        "            acc = np.mean(accuracy.iloc[indices])\n",
        "\n",
        "            # determine if there is a \"meaningful\" change - this is done with a stat sig cal\n",
        "            if prior_acc is not None and acc - prior_acc < acc_cutoff:\n",
        "                out.append((f'{col.name}:{p}-{prior_p}', acc - prior_acc, prior_indices - set(indices), acc))\n",
        "\n",
        "            # reset\n",
        "            prior_indices = set(indices)\n",
        "            prior_acc = acc\n",
        "            prior_p = p\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "########## Decision Tree #############\n",
        "\n",
        "def fit_DT(df, predictors = ['age']):\n",
        "    \"\"\" Fit a classification decision tree and return key elements \"\"\"\n",
        "\n",
        "    X = df[predictors] \n",
        "    y = df['accuracy_bool'] \n",
        "\n",
        "    model = DecisionTreeClassifier(max_depth=3, criterion='entropy', random_state=1)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    preds = model.predict(X)\n",
        "    acc = accuracy_score(y, preds)\n",
        "\n",
        "    return model, preds, acc, X\n",
        "\n",
        "def visualize_DT_1(df, model, predictors=['age'], fname='tree'):\n",
        "    \"\"\" Visualize and export DT model \"\"\"\n",
        "    data = export_graphviz(model, feature_names=predictors,\n",
        "                           filled=True, rounded=True, node_ids=True)\n",
        "\n",
        "    graph = graphviz.Source(data)\n",
        "    graph.render('trees/'+fname)\n",
        "\n",
        "def visualize_DT_2(df, model, predictors=['age'], fname='tree'):\n",
        "    \"\"\" Visualize tree with data plots \"\"\"\n",
        "    X = df[predictors]\n",
        "    acc = df['accuracy_bool'].values\n",
        "    viz = dtreeviz(model, X, acc,\n",
        "                    target_name=\"accuracy_bool\",\n",
        "                    feature_names=predictors,\n",
        "                    class_names=['True','False'] if acc[0] else ['False','True'])\n",
        "\n",
        "    viz.save('trees/'+fname+'.svg')\n",
        "\n",
        "\n",
        "def return_dt_split(model, col, accuracy, col_2=None, impurity_cutoff=1.0, n_datapoints_cutoff=5, acc_cutoff=0.03):\n",
        "    \"\"\"\n",
        "    Return all indices of col that meet the following criteria:\n",
        "    1. Leaf has accuracy lower that baseline by acc_cutoff\n",
        "    2. Split size > n_datapoints_cutoff \n",
        "    :param model: SKLearn classification decision tree model\n",
        "    :param col: (pd.Series) column used to split on\n",
        "    :param accuracy: (pd.Series) column corresponding to correct/incorrect classification\n",
        "    :param col_2: (pd.Series) column to be used for interactions\n",
        "    :param impurity_cutoff: (float) requirement for entropy/gini of leaf\n",
        "    :param n_datapoints_cutoff: (int) minimum n in a final node to be returned\n",
        "    :param acc_cutoff: (float) accuracy cutoff for returning float\n",
        "    :return: (dict[node_idx, indices]) where indices corresponds to the col that meet the above criteria\n",
        "    \"\"\"\n",
        "\n",
        "    # get leaf ids and setup\n",
        "    df = pd.concat([col, col_2], axis=1) if col_2 is not None else pd.DataFrame(col)\n",
        "    leaf_id = model.apply(df)\n",
        "\n",
        "    t = model.tree_\n",
        "    baseline_acc = np.mean(accuracy)\n",
        "\n",
        "    # get indices of leaf ids that meet criteria\n",
        "    keeps_1 = {i for i,v in enumerate(t.n_node_samples) if v > n_datapoints_cutoff} # sample size\n",
        "    keeps_2 = {i for i,v in enumerate(t.impurity) if v <= impurity_cutoff} # sample size\n",
        "    keeps = keeps_1 & keeps_2\n",
        "\n",
        "    # store all data and corresponding index\n",
        "    node_indices = {}\n",
        "    slice_acc = -1\n",
        "    for idx in keeps:\n",
        "        node_indices[idx] = [i for i,v in enumerate(leaf_id) if v == idx]\n",
        "\n",
        "        # remove non-low-accuracy areas and empty lists\n",
        "        slice_acc = [x[1] / sum(x) for x in t.value[idx]] \n",
        "        if baseline_acc - slice_acc < acc_cutoff or node_indices[idx] == []:\n",
        "            del node_indices[idx]\n",
        "            slice_acc = None\n",
        "\n",
        "    return (f'{col.name}{\"-\"+col_2.name if col_2 is not None else \"\"}', list(node_indices.keys()), list(node_indices.values()), slice_acc)\n",
        "\n",
        "\n",
        "###################### Run Helpers #############\n",
        "\n",
        "def run_data_search(df, viz=2, do_a=True, do_b=True, do_c=True):\n",
        "    \"\"\" Iterate over data columns and perform the following actions...\n",
        "    1. If type(col) is numeric, run HDP\n",
        "    2. If type(col) is categorical, run DT\n",
        "    3. Run DT for interactions\n",
        "    Save DT viz and print problematic indices\n",
        "    :param df: (pd.DataFrame) of raw data with correct/incorrect classification\n",
        "    :param viz: (int) 0 if no viz, 1 if viz type 1, 2 if vis type 2\n",
        "    \"\"\"\n",
        "\n",
        "    categoricals  = [x for x in list(df) if 'color' in x or 'gender' in x]\n",
        "    acc_col = df['accuracy_bool']\n",
        "\n",
        "    # store for output\n",
        "    univariate_numerics_acc = []\n",
        "    categoricals_acc = []\n",
        "    bivariate_acc = []\n",
        "\n",
        "    # univariate loop\n",
        "    for col_name in [x for x in list(df) if x != 'accuracy_bool']:\n",
        "        c = df[col_name]\n",
        "\n",
        "        if col_name not in categoricals and col_name not in ('outcome','accuracy_bool') and do_a:\n",
        "            print(f'Running: HDP for {col_name}')\n",
        "            univariate_numerics_acc.append(hpd_iterative_search(c, acc_col))\n",
        "\n",
        "        # categoricals\n",
        "        elif do_b:\n",
        "            print(f'Running: DT for {col_name}')\n",
        "            predictors = [col_name]\n",
        "            model, *_, X = fit_DT(df, predictors)\n",
        "\n",
        "            if viz == 1:\n",
        "                visualize_DT_1(df, model, predictors=predictors, fname=f'{col_name}')\n",
        "            elif viz == 2:\n",
        "                visualize_DT_2(df, model, predictors=predictors, fname=f'{col_name}')\n",
        "\n",
        "            categoricals_acc.append(return_dt_split(model, c, acc_col))\n",
        "\n",
        "    # bivariate loop (interactions)\n",
        "    if do_c:\n",
        "        for col1, col2 in itertools.combinations(set(df) - set(['accuracy_bool','outcome']), 2):\n",
        "            print(f'Running: DT for {col1} and {col2}')\n",
        "            c1, c2 = df[col1], df[col2]\n",
        "            predictors = [col1,col2]\n",
        "            model, *_, X = fit_DT(df, predictors=predictors)\n",
        "\n",
        "            if viz == 1:\n",
        "                visualize_DT_1(df, model, predictors=predictors, fname=f'{col1}-{col2}')\n",
        "            elif viz == 2:\n",
        "                visualize_DT_2(df, model, predictors=predictors, fname=f'{col1}-{col2}')\n",
        "\n",
        "            bivariate_acc.append(return_dt_split(model, c1, acc_col, c2))\n",
        "\n",
        "    return (univariate_numerics_acc, categoricals_acc, bivariate_acc)\n",
        "\n",
        "def clean_output(a, b, c):\n",
        "    \"\"\" \n",
        "    Take list of outputs of HPD, DT, and DT interactions and return sorted \n",
        "    value by accuracy drop.\n",
        "    \"\"\"\n",
        "\n",
        "    names, indices, accuracies, method = [], [], [], []\n",
        "\n",
        "    # Univarite using HPD\n",
        "\n",
        "    # Save vals \n",
        "    for v in a:\n",
        "        for x in v:\n",
        "            names.append(x[0])\n",
        "            indices.append(x[2])\n",
        "            accuracies.append(x[3])\n",
        "            method.append('HPD')\n",
        "\n",
        "    for x in b:\n",
        "        names.append(x[0])\n",
        "        indices.append(x[2])\n",
        "        accuracies.append(x[3])\n",
        "        method.append('DT')\n",
        "\n",
        "    for x in c:\n",
        "        names.append(x[0])\n",
        "        indices.append(x[2])\n",
        "        accuracies.append(x[3])\n",
        "        method.append('DT')\n",
        "\n",
        "    out = pd.DataFrame(dict(names=names, indicies=indices, accuracies=accuracies, method=method))\n",
        "    out.sort_values(by=['accuracies'], inplace=True)\n",
        "    out.index = range(len(out.index))\n",
        "\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "CcgEoYaXTqAy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "ad6b6f6f-bcee-48b7-f969-778b33a19461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9c5f8ad22806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_rows'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# XGBoost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################### Run #############\n",
        "\n",
        "# Step 1: train our baseline model \n",
        "df = boston.data\n",
        "df_test = pre_process_FreeAi(xgbr, X_train, y_train, threshold=0.1) # add preds to df\n",
        "\n",
        "# Step 2: run HPD Example\n",
        "#hpd_example(show_plot=False)\n",
        "\n",
        "# Step 3: run DT example\n",
        "# Step 3.1: univariate\n",
        "predictors=['CRIM']\n",
        "model, preds, acc, X = fit_DT(df_test, predictors=predictors)\n",
        "visualize_DT_1(df_test, model, predictors=predictors, fname=f'bivariate_demo')\n",
        "visualize_DT_2(df_test, model, predictors=predictors, fname=f'bivariate_demo')\n",
        "x = return_dt_split(model, df_test[predictors[0]], df_test['accuracy'])\n",
        "#print(x)\n",
        "\n",
        "# Step 3.2: bivariate\n",
        "predictors = ['glucose','blood_pressure']\n",
        "model, preds, acc, X = fit_DT(df_test, predictors=predictors)\n",
        "visualize_DT_1(df_test, model, predictors=predictors, fname=f'bivariate_demo')\n",
        "visualize_DT_2(df_test, model, predictors=predictors, fname=f'bivariate_demo')\n",
        "x = return_dt_split(model, df_test[predictors[0]], df_test['accuracy'], df_test[predictors[1]])\n",
        "#print(x)\n",
        "\n",
        "# Step 4: find areas of weakness\n",
        "a, b, c = run_data_search(df_test)\n",
        "out = clean_output(a,b,c)\n",
        "out.dropna(inplace=True)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "LyImu3wOYDBg",
        "outputId": "a11d3c60-3a7a-446e-db99-5d47a3ec2899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a3f2909b0e7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Step 1: train our baseline model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboston\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process_FreeAi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgbr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add preds to df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Step 2: run HPD Example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pre_process_FreeAi' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DT\n",
        "import graphviz\n",
        "from dtreeviz.trees import dtreeviz\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz, _tree\n",
        "import itertools\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "def fit_DT(df):\n",
        "    \"\"\" Fit a classification decision tree and return key elements \"\"\"\n",
        "\n",
        "    X = df[:-1] \n",
        "    y = df[-1] \n",
        "\n",
        "    model = DecisionTreeClassifier(max_depth=3, criterion='entropy', random_state=1)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    preds = model.predict(X)\n",
        "    acc = accuracy_score(y, preds)\n",
        "\n",
        "    return model, preds, acc, X\n",
        "\n",
        "def visualize_DT_1(df, model, predictors=['age'], fname='tree'):\n",
        "    \"\"\" Visualize and export DT model \"\"\"\n",
        "    data = export_graphviz(model, feature_names=predictors,\n",
        "                           filled=True, rounded=True, node_ids=True)\n",
        "\n",
        "    graph = graphviz.Source(data)\n",
        "    graph.render('trees/'+fname)\n",
        "\n",
        "def visualize_DT_2(df, model, predictors=['age'], fname='tree'):\n",
        "    \"\"\" Visualize tree with data plots \"\"\"\n",
        "    X = df[predictors]\n",
        "    acc = df['accuracy_bool'].values\n",
        "    viz = dtreeviz(model, X, acc,\n",
        "                    target_name=\"accuracy_bool\",\n",
        "                    feature_names=predictors,\n",
        "                    class_names=['True','False'] if acc[0] else ['False','True'])\n",
        "\n",
        "    viz.save('trees/'+fname+'.svg')\n"
      ],
      "metadata": {
        "id": "gruuY2YiLJd8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}